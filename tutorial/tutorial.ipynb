{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdjSim Tutorial\n",
    "\n",
    "AdjSim is an agent-based modelling engine. It allows users to define simulation environments through which agents interact through ability casting and timestep iteration. It is tailored towards allowing agents to behave intelligently through the employment of Reinforcement Learning techniques such as Q-Learning. \n",
    "\n",
    "This tutorial will enumerate many of AdjSim's features through the construction of a simulation where a group of agents will play a game of tag. Lets dive in!\n",
    "\n",
    "---\n",
    "\n",
    "## The Simulation Space\n",
    "\n",
    "AdjSim simulations take place inside the fittingly named `Simulation` object.\n",
    "\n",
    "Lets begin by importing our libararies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import adjsim\n",
    "import numpy as np # AdjSim also heavily relies on numpy. Its usage is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a trivial empty simulation to display its usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating timestep 100/100 - population: 0"
     ]
    }
   ],
   "source": [
    "sim = adjsim.core.Simulation()\n",
    "\n",
    "# Interactive simulation method.\n",
    "sim.start() # Begin the simulation.\n",
    "sim.step() # Take one step.\n",
    "sim.step(10) # Take 10 steps.\n",
    "sim.end() # end the simulation.\n",
    "\n",
    "# Batch simulation method.\n",
    "sim.simulate(100) # Start, Simulate for 100 timesteps, End. All in one go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its a little lonely in that simulation, lets add an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating timestep 100/100 - population: 2 "
     ]
    }
   ],
   "source": [
    "sim.agents.add(adjsim.core.Agent())\n",
    "sim.agents.add(adjsim.core.Agent())\n",
    "\n",
    "sim.simulate(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things are happening, but its a little hard to see. Lets visualize our simulation space. Simulations and agents have a hierarchal class structure. `VisualSimulation` inherits from `Simulation`, and can be used to display a 2D simulation environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating timestep 15/15 - population: 0 "
     ]
    }
   ],
   "source": [
    "sim = adjsim.core.VisualSimulation()\n",
    "sim.simulate(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, an appropriate `VisualAgent` must be present in the simulation space if it is to be seen. The agent inheritance hierarchy is as follows:\n",
    "\n",
    "1. `Agent`: The base agent class. Contains the attributes that are minimally needed in order for an agent to be simulated.\n",
    "2. `SpatialAgent`: Adds the presence of the `pos` Attribute, allowing for a 2D position to be associated with an agent.\n",
    "3. `VisualAgent`: Adds the attributes needed for visualization on top of those of `SpatialAgent`. This is the minimum class needed for an agent to appear inside a `VisualSimulation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating timestep 15/15 - population: 1 "
     ]
    }
   ],
   "source": [
    "sim.agents.add(adjsim.core.VisualAgent(pos=np.array([1,1])))\n",
    "sim.simulate(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should see our agent! Next, lets make it do things.\n",
    "\n",
    "---\n",
    "\n",
    "## Agents\n",
    "\n",
    "Agents are simulation artifacts that manipulate the simulation environment. In the adjsim model, within each timestep agents take turns acting out their environment manipulations. One iteration of an agent's environment manipulations is known as an agent's __step__.\n",
    "\n",
    "There are two major aspects to an agent's __step__: \n",
    "\n",
    "1. A set of __actions__. An __action__ defines one distinct user-defined set of computations that the agent manipulates its environment with. It is simply a python function. The actions we will be defining in our simulation of the game of tag will be a _move_ and a _tag_ action.\n",
    "2. A decision module. Decision modules are AdjSim objects that an agent uses to choose which actions to perform, and in what order.\n",
    "\n",
    "We'll start by making an agent that can _move_, but that can't _tag_. The _move_ action will allow an agent to move in a random direction bounded by a square arena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants.\n",
    "ARENA_BOUND = 100\n",
    "MOVE_DIST = 20\n",
    "\n",
    "# Actions must follos the following signiature. \n",
    "# The Simulation object will be passed in as the 'simulation' parameter.\n",
    "# The agent calling the action will be passed as the 'source' parameter.\n",
    "def move(simulation, source):\n",
    "    movement = (np.random.rand(2) - 0.5) * MOVE_DIST\n",
    "    source.pos = np.clip(source.pos + movement, -ARENA_BOUND, ARENA_BOUND)\n",
    "    source.step_complete = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final line, `source.step_complete = True`, lets the decision module know that no further actions can be completed in the agent's __step__ after the current one. \n",
    "\n",
    "Now let's create an agent that uses this action. The agent we will be creating will have a `RandomRepeatedCastDecision`. This decision module will randomly select actions and invoke them until the `source.step_complete` attribute is true. This should result in one cast of the above-defined _move_ action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Mover(adjsim.core.VisualAgent):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__(pos=np.array([x, y]))\n",
    "        \n",
    "        # Set the decision module.\n",
    "        self.decision = adjsim.decision.RandomSingleCastDecision()\n",
    "\n",
    "        # Populate the agent's action list.\n",
    "        self.actions[\"move\"] = move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take it out for a spin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating timestep 15/15 - population: 1 "
     ]
    }
   ],
   "source": [
    "sim = adjsim.core.VisualSimulation()\n",
    "sim.agents.add(Mover(0, 0))\n",
    "\n",
    "sim.simulate(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an agent moving in random directions in each timestep. Nice! Lets give it some friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating timestep 15/15 - population: 25 "
     ]
    }
   ],
   "source": [
    "class MoverSimulation(adjsim.core.VisualSimulation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                self.agents.add(Mover(20*i, 20*j))\n",
    "                \n",
    "sim = MoverSimulation()\n",
    "sim.simulate(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now see a group of 25 agents moving in a similar fashion to our first one. The basics are now covered. Lets put together our simulation of a game of tag.\n",
    "\n",
    "---\n",
    "\n",
    "## Putting It All Together\n",
    "\n",
    "The following will describe our simulation of a game of tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating timestep 15/15 - population: 26 "
     ]
    }
   ],
   "source": [
    "# Reiterate imports.\n",
    "import adjsim\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Constants.\n",
    "ARENA_BOUND = 100\n",
    "TAG_DIST_SQUARE = 100\n",
    "MOVE_DIST = 20\n",
    "\n",
    "def move(simulation, source):\n",
    "    movement = (np.random.rand(2) - 0.5) * MOVE_DIST\n",
    "    source.pos = np.clip(source.pos + movement, -ARENA_BOUND, ARENA_BOUND)\n",
    "    source.step_complete = True\n",
    "\n",
    "def tag(simulation, source):  \n",
    "    if not source.is_it:\n",
    "        return\n",
    "\n",
    "    # Find nearest neighbour.\n",
    "    closest_distance = sys.float_info.max\n",
    "    nearest_neighbour = None\n",
    "    for agent in simulation.agents:\n",
    "        if agent.id == source.id:\n",
    "            continue\n",
    "\n",
    "        distance = adjsim.utility.distance_square(agent, source)\n",
    "        if distance < closest_distance:\n",
    "            nearest_neighbour = agent\n",
    "            closest_distance = distance\n",
    "\n",
    "    if closest_distance > TAG_DIST_SQUARE:\n",
    "        return\n",
    "\n",
    "    assert nearest_neighbour\n",
    "\n",
    "    # Perform Tag.\n",
    "    nearest_neighbour.is_it = True\n",
    "    nearest_neighbour.color = adjsim.color.RED_DARK # This will change the agent's visual color.\n",
    "    nearest_neighbour.order = 1 # Order describes what order the agents will take their steps in the simulation loop.\n",
    "    source.is_it = False\n",
    "    source.order = 0 # Order describes what order the agents will take their steps in the simulation loop.\n",
    "    source.color = adjsim.color.BLUE_DARK # This will change the agent's visual color.\n",
    "\n",
    "class Tagger(adjsim.core.VisualAgent):\n",
    "\n",
    "    def __init__(self, x, y, is_it):\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_it = is_it\n",
    "        self.color = adjsim.color.RED_DARK if is_it else adjsim.color.BLUE_DARK\n",
    "        self.pos = np.array([x, y])\n",
    "        self.decision = adjsim.decision.RandomSingleCastDecision()\n",
    "\n",
    "        self.actions[\"move\"] = move\n",
    "        self.actions[\"tag\"] = tag\n",
    "\n",
    "        if is_it:\n",
    "            self.order = 1 # Order describes what order the agents will take their steps in the simulation loop.\n",
    "\n",
    "\n",
    "class TaggerSimulation(adjsim.core.VisualSimulation):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                self.agents.add(Tagger(20*i, 20*j, False))\n",
    "\n",
    "        self.agents.add(Tagger(10, 10, True))\n",
    "        \n",
    "sim = TaggerSimulation()\n",
    "sim.simulate(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We should observe a game of tag being played where the 'it' agent is red. It will tag the agents around it when it has the chance. \n",
    "\n",
    "---\n",
    "\n",
    "## Filling the Noggin\n",
    "\n",
    "The agents so far act randomly. Let's fix that.\n",
    "\n",
    "### Using Reinforcement Learning-Enabled Decision Models\n",
    "\n",
    "In this part of the tutorial we will be exploring AdjSim's Functional Decision model. This is the architecture that is used by the decision modules that employ Reinforcement Learning within AdjSim. \n",
    "\n",
    "The model asserts that there are two additional pieces of information that need to be given to an agent in order for it to be able to make intelligent decisions:\n",
    "\n",
    "1. A __Loss Function__: This function essentially gives an agent a score as to how well its doing. Functional decision modules attempy to __minimize__ their associated loss function. For example, taggers try to minimize the amount of time they spend tagged, or bacteria try to maximize their calories.\n",
    "2. A __Perception Function__: A function that processes the simulation from the perspective of an agent, and returns an object that is then used as input to the decision module for the purposes of invoking intelligent actions. For example, a given `Tagger` should be aware of where the `Tagger` that is 'it' is located relative to itsef. The perception acts as a filter between the omniscient data present within a simulation and the data that an agent may use to make its decisions. An invocation of a __Perception__ at a particular timestep is known as a __Observation__.\n",
    "\n",
    "\n",
    "\n",
    "Overall, the processing workflow in an agent __step__ is the following.\n",
    "\n",
    "$$\n",
    "\\text{Simulation} \\rightarrow \\text{Perception Function} \\rightarrow \\text{(Observation)} \\rightarrow \\text{Decision Module}\n",
    "$$\n",
    "\n",
    "Then the decision module is responsible for selecting and invoking the appropriate actions.\n",
    "\n",
    "$$\n",
    "\\text{Decision Module} \\rightarrow \\text{Actions} \\rightarrow \\text{(Environment Manipulations)}\n",
    "$$\n",
    "\n",
    "Finally, the decision module evaluates the efficacy of its actions through the __Loss Function__ evaluation. It uses these results to train itself to make better future decisions.\n",
    "\n",
    "$$\n",
    "\\text{Decision Module} \\rightarrow \\text{Loss Function} \\rightarrow \\text{(Efficacy Evaluation Score)} \\rightarrow \\text{Decision Module}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "We're going to start by making the agents of our simulation choose the right action between moving and tagging. Note that the direction of movement is still random. Parameterized functions will be covered in the next section. For our __Loss Function__, we will be using the `is_it` attribute to discourage being tagged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call signiature must be the following, same as an action.\n",
    "def tagger_loss(simulation, source):\n",
    "    return 10 if source.is_it else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our __Perception Function__ will return two parameters:\n",
    "\n",
    "1. Whether or not the agent is 'it'\n",
    "2. The relative location of the nearest agent if the agent is 'it', otherwise the relative location of the 'it' agent. This will be done in polar coordinates.\n",
    "\n",
    "Since we will be using Q-Learning for this simulation, each __Observation__ will define a state of the agent. Since relative location involves continuous values (floats), we find ourselves with an infinite number of possible agent states. In order to make the algorithm palpable we will discretize the values obtained that would otherwise be returned from the perception function. This need to discretize states is a idiosyncracy of the Q-Learning algorithm, and may not be needed when other decision modules are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def find_it_tagger(simulation, source):\n",
    "    for agent in simulation.agents:\n",
    "        if agent.is_it:\n",
    "            return (agent, adjsim.utility.distance_square(agent, source))\n",
    "        \n",
    "    # Raise an error if not found, this should never happen.\n",
    "    raise Exception(\"'it' agent not found.\")\n",
    "\n",
    "def find_closest_non_it_tagger(simulation, source):\n",
    "    closest_distance = sys.float_info.max\n",
    "    nearest_neighbour = None\n",
    "    for agent in simulation.agents:\n",
    "        if agent.id == source.id:\n",
    "            continue\n",
    "\n",
    "        distance = adjsim.utility.distance_square(agent, source)\n",
    "        if distance < closest_distance:\n",
    "            nearest_neighbour = agent\n",
    "            closest_distance = distance\n",
    "\n",
    "    return (nearest_neighbour, closest_distance)\n",
    "\n",
    "def tagger_perception(simulation, source):\n",
    "    distance = None\n",
    "    agent = None\n",
    "    \n",
    "    # Find appropriate agent and distance.\n",
    "    if source.is_it:\n",
    "        agent, distance = find_closest_non_it_tagger(simulation, source)\n",
    "    else:\n",
    "        agent, distance = find_it_tagger(simulation, source)\n",
    "\n",
    "    # Obtain theta value.\n",
    "    delta = agent.pos - source.pos\n",
    "    theta = math.atan(delta[1]/delta[0]) if delta[0] != 0 else np.sign(delta[1])*math.pi/2\n",
    "\n",
    "    # Discretize observation to reduce number of possible states.\n",
    "    rounded_distance = round(distance/40)*40\n",
    "    rounded_theta = round(theta/5)*5\n",
    "\n",
    "    return (rounded_distance, rounded_theta, source.is_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's put together our simulation. It is important to note that we will be sharing our decision module object across all our Taggers in the same way that actions are shared. This will allow training to occur collectively, and allow all agents to learn from each other's enterprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Learning training data found, loading from somewhat_clever_tagger.qlearning.pkl...done\n",
      "Simulating timestep 15/15 - population: 26 Processing Q-Learning data...done\n",
      "Saving Q-Learning data...done\n"
     ]
    }
   ],
   "source": [
    "class SomewhatCleverTagger(adjsim.core.VisualAgent):\n",
    "\n",
    "    def __init__(self, x, y, is_it, _decision):\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_it = is_it\n",
    "        self.color = adjsim.color.RED_DARK if is_it else adjsim.color.BLUE_DARK\n",
    "        self.pos = np.array([x, y])\n",
    "        self.decision = _decision\n",
    "\n",
    "        self.actions[\"move\"] = move\n",
    "        self.actions[\"tag\"] = tag\n",
    "\n",
    "        if is_it:\n",
    "            self.order = 1\n",
    "\n",
    "\n",
    "class SomewhatCleverTaggerSimulation(adjsim.core.VisualSimulation):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Let's create the collective decision module. We will load and save our progress to the same file.\n",
    "        io_file_name = \"somewhat_clever_tagger.qlearning.pkl\"\n",
    "        self.tagger_decision = adjsim.decision.QLearningDecision(perception=tagger_perception, loss=tagger_loss, \n",
    "            callbacks=self.callbacks, input_file_name=io_file_name, output_file_name=io_file_name)\n",
    "\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                self.agents.add(SomewhatCleverTagger(20*i, 20*j, False, self.tagger_decision))\n",
    "\n",
    "        self.agents.add(SomewhatCleverTagger(10, 10, True, self.tagger_decision))\n",
    "        \n",
    "sim = SomewhatCleverTaggerSimulation()\n",
    "sim.simulate(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we are prepared to run the simulation a series of times, and after training the bacteria will be able to choose _which_ of _move_ or _tag_ they should choose. We will go into more detail regarding the training/testing cycle in the next example, where the results will be more flashy.\n",
    "\n",
    "---\n",
    "\n",
    "### Parameterizing Actions Using Decision-Mutable Values\n",
    "\n",
    "We not only want our taggers to know which action to perform (between _move_ and _tag_), but also what direction to move when they perform their _move_ action. We accomplish this using Decision-Mutable Values. These are variable that a decision module will try to optimize for when performing its training cycle. These values are never explicitly set by the user. The decision module will set its value during runtime, before calling each action. These values are effectively read-only for the consumer of the API. \n",
    "\n",
    "Let's take a look. We will begin by re-writing our _move_ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def move(simulation, source):\n",
    "    # We need to convert polar movement coordinates to cartesian.\n",
    "    move_rho = source.move_rho.value\n",
    "    move_theta = source.move_theta.value\n",
    "\n",
    "    dx = math.cos(move_theta) * move_rho\n",
    "    dy = math.sin(move_theta) * move_rho\n",
    "\n",
    "    movement = np.array([dx, dy])\n",
    "\n",
    "    source.pos = source.pos + movement\n",
    "    source.step_complete = True\n",
    "        \n",
    "        \n",
    "class ProperlyCleverTagger(adjsim.core.VisualAgent):\n",
    "    def __init__(self, x, y, is_it, _decision):\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_it = is_it\n",
    "        self.color = adjsim.color.RED_DARK if is_it else adjsim.color.BLUE_DARK\n",
    "        self.pos = np.array([x, y])\n",
    "        self.decision = _decision\n",
    "        \n",
    "        # This is where the new magic lies. \n",
    "        self.move_rho = adjsim.decision.DecisionMutableFloat(0, MOVE_DIST)\n",
    "        self.move_theta = adjsim.decision.DecisionMutableFloat(0, 360)\n",
    "\n",
    "        self.actions[\"move\"] = move\n",
    "        self.actions[\"tag\"] = tag\n",
    "\n",
    "        if is_it:\n",
    "            self.order = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have properly clever taggers. Let's train them. We will make a train simulation and a test simulation, and run the training for a number of epochs. Let's see how they do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Learning training data found, loading from somewhat_clever_tagger.qlearning.pkl...done\n",
      "Simulating timestep 300/300 - population: 26 Processing Q-Learning data...done\n",
      "Saving Q-Learning data...done\n",
      "Q Learning training data found, loading from somewhat_clever_tagger.qlearning.pkl...done\n",
      "Simulating timestep 300/300 - population: 26 Processing Q-Learning data...done\n",
      "Saving Q-Learning data...done\n",
      "Q Learning training data found, loading from somewhat_clever_tagger.qlearning.pkl...done\n",
      "Simulating timestep 300/300 - population: 26 Processing Q-Learning data...done\n",
      "Saving Q-Learning data...done\n",
      "Q Learning training data found, loading from somewhat_clever_tagger.qlearning.pkl...done\n",
      "Simulating timestep 300/300 - population: 26 Processing Q-Learning data...done\n",
      "Saving Q-Learning data...done\n",
      "Q Learning training data found, loading from somewhat_clever_tagger.qlearning.pkl...done\n",
      "Simulating timestep 300/300 - population: 26 Processing Q-Learning data...done\n",
      "Saving Q-Learning data...done\n",
      "Q Learning training data found, loading from somewhat_clever_tagger.qlearning.pkl...done\n",
      "Simulating timestep 300/300 - population: 26 Processing Q-Learning data...done\n",
      "Saving Q-Learning data...done\n",
      "Q Learning training data found, loading from somewhat_clever_tagger.qlearning.pkl...done\n",
      "Simulating timestep 300/300 - population: 26 Processing Q-Learning data...done\n",
      "Saving Q-Learning data...done\n",
      "Q Learning training data found, loading from somewhat_clever_tagger.qlearning.pkl...done\n",
      "Simulating timestep 300/300 - population: 26 Processing Q-Learning data...done\n",
      "Saving Q-Learning data...done\n",
      "Q Learning training data found, loading from somewhat_clever_tagger.qlearning.pkl...done\n",
      "Simulating timestep 300/300 - population: 26 Processing Q-Learning data...done\n",
      "Saving Q-Learning data...done\n",
      "Q Learning training data found, loading from somewhat_clever_tagger.qlearning.pkl...done\n",
      "Simulating timestep 300/300 - population: 26 Processing Q-Learning data...done\n",
      "Saving Q-Learning data...done\n",
      "Q Learning training data found, loading from somewhat_clever_tagger.qlearning.pkl...done\n",
      "Simulating timestep 32/50 - population: 26 Processing Q-Learning data...done\n",
      "Saving Q-Learning data...done\n"
     ]
    }
   ],
   "source": [
    "class ProperlyCleverTaggerTrainSimulation(adjsim.core.Simulation): # Note that the simulation is not visual for training.\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        io_file_name = \"properly_clever_tagger.qlearning.pkl\"\n",
    "        self.tagger_decision = adjsim.decision.QLearningDecision(perception=tagger_perception, loss=tagger_loss, \n",
    "            callbacks=self.callbacks, input_file_name=io_file_name, output_file_name=io_file_name)\n",
    "\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                self.agents.add(ProperlyCleverTagger(20*i, 20*j, False, self.tagger_decision))\n",
    "\n",
    "        self.agents.add(ProperlyCleverTagger(10, 10, True, self.tagger_decision))\n",
    "        \n",
    "class ProperlyCleverTaggerTestSimulation(adjsim.core.VisualSimulation): # Note that the simulation is not visual for training.\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        io_file_name = \"properly_clever_tagger.qlearning.pkl\"\n",
    "        self.tagger_decision = adjsim.decision.QLearningDecision(perception=tagger_perception, loss=tagger_loss, \n",
    "            callbacks=self.callbacks, input_file_name=io_file_name, output_file_name=io_file_name, nonconformity_probability=0)\n",
    "\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                self.agents.add(ProperlyCleverTagger(20*i, 20*j, False, self.tagger_decision))\n",
    "\n",
    "        self.agents.add(ProperlyCleverTagger(10, 10, True, self.tagger_decision))\n",
    "        \n",
    "sim = ProperlyCleverTaggerTestSimulation()\n",
    "sim.simulate(50)\n",
    "\n",
    "# Training cycle.\n",
    "EPOCHS = 10\n",
    "for i in range(EPOCHS):\n",
    "    sim = ProperlyCleverTaggerTrainSimulation()\n",
    "    sim.simulate(300)\n",
    "    \n",
    "# Testing Cycle.\n",
    "sim = ProperlyCleverTaggerTestSimulation()\n",
    "sim.simulate(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
